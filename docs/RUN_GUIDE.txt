AI IMAGE CLASSIFICATION SERVICE WITH SDN-BASED LOAD BALANCING
============================================================

🚀 COMPREHENSIVE RUN GUIDE 🚀

This guide provides step-by-step instructions to set up, run, demo, test, and troubleshoot the AI Image Classification Service with SDN-based Load Balancing.

═══════════════════════════════════════════════════════════════════════════════

📋 TABLE OF CONTENTS
===================

1. PREREQUISITES
2. QUICK START (RECOMMENDED)
3. DETAILED SETUP PATHS
4. USING THE WEB UI
5. CONFIGURATION & CUSTOMIZATION
6. LOAD TESTING & PERFORMANCE
7. OBSERVABILITY & MONITORING
8. FAILURE DEMONSTRATION
9. TROUBLESHOOTING
10. TALKING POINTS FOR FACULTY/INTERVIEWS
11. FUTURE ENHANCEMENTS

═══════════════════════════════════════════════════════════════════════════════

1. PREREQUISITES
================

🖥️ System Requirements:
- Operating System: Linux, macOS, or Windows with WSL2
- RAM: Minimum 8GB, Recommended 16GB
- Disk Space: At least 10GB free
- CPU: Multi-core processor (for better model performance)

📦 Software Requirements:
- Docker: Version 20.10+ with Docker Compose V2
- Python: 3.8+ (for local development and testing)
- Node.js: 16+ (optional, only if developing React frontend)
- Git: For cloning and version control

🔗 Optional (for advanced features):
- Mininet: For SDN network emulation
- Kubernetes: For K8s deployment
- kubectl: For K8s management

Installation Commands:

# Ubuntu/Debian
sudo apt update
sudo apt install docker.io docker-compose python3 python3-pip git

# macOS (with Homebrew)
brew install docker docker-compose python3 git

# Windows (WSL2)
# Install Docker Desktop for Windows
# Install Python from python.org

Verify Installation:
docker --version          # Should show 20.10+
docker-compose --version  # Should show v2+
python3 --version         # Should show 3.8+

═══════════════════════════════════════════════════════════════════════════════

2. QUICK START (RECOMMENDED)
=============================

🚀 Get up and running in 5 minutes!

Step 1: Clone the Repository
----------------------------
git clone <repository-url>
cd Cloud

Step 2: Start the Complete System
----------------------------------
# Make scripts executable
chmod +x scripts/*.sh

# Build and start all services
./scripts/run_compose.sh

This script will:
- Build all Docker images
- Start all services (inference, load balancer, frontend, observability)
- Check service health
- Display access URLs

Step 3: Access the Web Interface
--------------------------------
Open your browser and navigate to:
🌐 http://localhost:3000

Step 4: Test Image Classification
----------------------------------
1. Upload an image using drag & drop or file browser
2. Click "🔍 Classify Image"
3. View results with confidence, server info, and latency
4. Check server statistics showing load distribution

Step 5: Explore Additional Interfaces
--------------------------------------
🤖 Load Balancer API: http://localhost:8080/api/server-stats
📈 Prometheus Metrics: http://localhost:9090
📊 Grafana Dashboards: http://localhost:3001 (admin/admin123)

═══════════════════════════════════════════════════════════════════════════════

3. DETAILED SETUP PATHS
========================

Path A: Docker Compose (Production-like)
=========================================

This is the recommended approach for full system demonstration.

Build Services:
./scripts/build_all.sh

Start Services:
docker-compose up -d

Monitor Logs:
docker-compose logs -f

Stop Services:
docker-compose down

Path B: Minimal Local Development
==================================

For development and debugging individual components.

1. Start Inference Service Only:
--------------------------------
cd inference_service
pip install -r requirements.txt
python -m app.server

Access: http://localhost:5000

2. Start Load Balancer Only:
-----------------------------
cd controller
pip install -r requirements.txt
python -m ryu.cmd.manager ryu_lb_basic.py

Access: http://localhost:8080

3. Serve Frontend Statically:
------------------------------
cd frontend/static_version
python -m http.server 8000

Access: http://localhost:8000

Path C: Mininet + SDN + Multi-server
=====================================

For complete SDN network emulation (requires Mininet installation).

1. Install Mininet:
-------------------
# Ubuntu/Debian
sudo apt install mininet

# Or build from source
git clone https://github.com/mininet/mininet
cd mininet
util/install.sh -a

2. Run Mininet Topology:
------------------------
cd mininet
sudo python topology.py

3. Start Controllers and Services:
----------------------------------
# In separate terminals:
./scripts/run_compose.sh

4. Test SDN Flow Installation:
------------------------------
# In Mininet CLI:
h1 ping h2
h1 curl http://10.0.0.100/predict

═══════════════════════════════════════════════════════════════════════════════

4. USING THE WEB UI
===================

🌐 Web Interface Features
=========================

Upload Section:
- Drag & drop images directly onto the upload zone
- Click "browse files" for traditional file selection
- Supports JPG, PNG, GIF up to 10MB
- Real-time image preview before classification

Classification Results:
- Predicted label with confidence percentage
- Visual confidence bar (color-coded)
- Server ID that processed the request
- Model version used (v1/v2 for canary testing)
- Request latency in milliseconds

Test Gallery:
- Quick-test buttons for sample images
- Cat, Dog, Car, Flower, Food samples
- One-click classification for demos

Server Statistics:
- Current load balancing algorithm
- Total requests processed
- Per-server request distribution with visual bars
- Server health status (green/red indicators)
- Real-time updates every 3 seconds

Additional Features:
- Dark/light mode toggle (🌓 button)
- Copy JSON results to clipboard
- Repeat last classification
- Error notifications with auto-dismiss
- Responsive design for mobile devices

🎯 Demo Workflow
================

1. Basic Classification Demo:
   - Upload a clear image (cat, dog, vehicle)
   - Show prediction confidence and server selection
   - Repeat with different images

2. Load Balancing Demo:
   - Upload multiple images in succession
   - Watch server statistics update
   - Show request distribution across servers

3. Server Health Demo:
   - Stop one inference container: docker stop ai-inference-1
   - Refresh statistics to show unhealthy server
   - Upload image to show traffic redirected to healthy servers
   - Restart container: docker start ai-inference-1

4. Algorithm Switching Demo:
   - Change algorithm via API:
     curl -X POST http://localhost:8080/api/algorithm \
          -H "Content-Type: application/json" \
          -d '{"algorithm": "least_connections"}'
   - Upload images and observe different distribution patterns

═══════════════════════════════════════════════════════════════════════════════

5. CONFIGURATION & CUSTOMIZATION
=================================

🔧 Environment Variables
========================

Inference Service:
- MODEL_NAME: Model to use (default: MobileNetV2)
- MODEL_VERSION: Version identifier (default: v1)
- SERVER_ID: Server identifier (default: hostname:5000)
- PORT: Service port (default: 5000)
- DEBUG: Enable debug mode (default: False)

Load Balancer:
- BACKEND_SERVERS: Comma-separated server list (default: localhost:5001,localhost:5002)
- API_PORT: Management API port (default: 8080)
- VIRTUAL_IP: Virtual IP for SDN (default: 10.0.0.100)

Frontend:
- API_BASE_URL: Inference service URL (default: http://localhost:5000)
- LB_API_URL: Load balancer API URL (default: http://localhost:8080)

🎛️ Load Balancing Algorithms
=============================

Switch algorithms via API:

Round Robin:
curl -X POST http://localhost:8080/api/algorithm \
     -H "Content-Type: application/json" \
     -d '{"algorithm": "round_robin"}'

Least Connections:
curl -X POST http://localhost:8080/api/algorithm \
     -H "Content-Type: application/json" \
     -d '{"algorithm": "least_connections"}'

Latency Weighted:
curl -X POST http://localhost:8080/api/algorithm \
     -H "Content-Type: application/json" \
     -d '{"algorithm": "latency_weighted"}'

Weighted Round Robin:
curl -X POST http://localhost:8080/api/algorithm \
     -H "Content-Type: application/json" \
     -d '{"algorithm": "weighted_round_robin"}'

🚀 Canary Model Deployment
===========================

Enable model v2 canary testing:

1. Modify docker-compose.yml:
   environment:
     - MODEL_VERSION=v2

2. Restart specific service:
   docker-compose restart inference3

3. Monitor version distribution in web UI

═══════════════════════════════════════════════════════════════════════════════

6. LOAD TESTING & PERFORMANCE
==============================

🔥 Locust Load Testing
======================

1. Install Locust:
cd load_test
pip install -r requirements.txt

2. Start Load Test:
locust -f locustfile.py --host=http://localhost:5000

3. Open Locust Web UI:
http://localhost:8089

4. Configure Test:
- Number of users: 10-100
- Spawn rate: 5-10 users/second
- Duration: 5-10 minutes

5. Monitor Performance:
- Grafana dashboards: http://localhost:3001
- Prometheus metrics: http://localhost:9090
- Real-time statistics in web UI

📊 Performance Measurement Procedure
====================================

Baseline Test:
1. Start system with 3 inference servers
2. Run load test with 50 concurrent users
3. Record metrics:
   - Average response time
   - 95th percentile latency
   - Requests per second
   - Server CPU/memory usage

Algorithm Comparison:
1. Test each load balancing algorithm separately
2. Use identical load patterns
3. Compare request distribution and latency

Expected Results:
- Response time: < 2 seconds average
- Throughput: 10-50 requests/second (depends on hardware)
- CPU usage: 30-70% per inference container
- Memory usage: < 2GB per container

Sample Performance Results:
╭─────────────────┬─────────────┬────────────────┬──────────────╮
│ Algorithm       │ Avg Latency │ 95th Percentile│ Requests/sec │
├─────────────────┼─────────────┼────────────────┼──────────────┤
│ Round Robin     │ 1.2s        │ 2.1s           │ 45/s         │
│ Least Conn      │ 1.1s        │ 1.9s           │ 48/s         │
│ Latency Weight  │ 1.0s        │ 1.8s           │ 52/s         │
│ Weighted RR     │ 1.1s        │ 2.0s           │ 47/s         │
╰─────────────────┴─────────────┴────────────────┴──────────────╯

═══════════════════════════════════════════════════════════════════════════════

7. OBSERVABILITY & MONITORING
==============================

📈 Grafana Dashboards
======================

Access: http://localhost:3001
Login: admin / admin123

Available Dashboards:
1. Inference Overview
   - Request rate and latency trends
   - Model accuracy metrics
   - Server resource utilization

2. Load Balancer Performance
   - Algorithm effectiveness
   - Server selection distribution
   - Health check status

Key Metrics to Monitor:
- inference_requests_total: Total requests per server
- inference_latency_seconds: Request processing time
- lb_requests_total: Load balancer request distribution
- lb_server_health: Server health status (1=healthy, 0=unhealthy)

📊 Prometheus Queries
=====================

Access: http://localhost:9090

Useful Queries:

Request Rate:
rate(inference_requests_total[5m])

Average Latency:
histogram_quantile(0.95, rate(inference_latency_seconds_bucket[5m]))

Server Health:
lb_server_health

Load Distribution:
sum by (server_id) (rate(lb_requests_total[5m]))

🔍 Log Analysis
===============

View Real-time Logs:
docker-compose logs -f

Service-specific Logs:
docker-compose logs -f inference1
docker-compose logs -f load-balancer
docker-compose logs -f frontend

Filter Error Logs:
docker-compose logs | grep ERROR

═══════════════════════════════════════════════════════════════════════════════

8. FAILURE DEMONSTRATION
=========================

💥 Planned Failure Scenarios
============================

Scenario 1: Server Failure
---------------------------
1. Check current server stats in web UI
2. Stop one server:
   docker stop ai-inference-2
3. Wait 30 seconds for health check
4. Upload images and observe:
   - Server marked as unhealthy (red indicator)
   - Traffic redirected to healthy servers
   - No user-facing errors
5. Restart server:
   docker start ai-inference-2
6. Verify recovery in web UI

Scenario 2: Load Balancer Failure
----------------------------------
1. Stop load balancer:
   docker stop ai-load-balancer
2. Observe:
   - Web UI statistics stop updating
   - Direct server access still works
3. Restart load balancer:
   docker start ai-load-balancer
4. Verify statistics resume

Scenario 3: High Load Stress Test
----------------------------------
1. Start aggressive load test:
   locust -f load_test/locustfile.py --host=http://localhost:5000 --users=200 --spawn-rate=20
2. Monitor in Grafana:
   - CPU/memory spikes
   - Latency increases
   - Potential server overload
3. Observe automatic load balancing adjustments

Scenario 4: Network Partition (with Mininet)
---------------------------------------------
1. In Mininet CLI:
   link s1 h2 down
2. Observe server isolation
3. Restore link:
   link s1 h2 up
4. Verify recovery

🛠️ Recovery Scripts
===================

Automated Recovery:
./scripts/demo_failover.sh

This script demonstrates:
1. Normal operation baseline
2. Planned server failure
3. Automatic traffic redistribution
4. Server recovery
5. Performance metrics comparison

═══════════════════════════════════════════════════════════════════════════════

9. TROUBLESHOOTING
==================

🚨 Common Issues and Solutions
==============================

Issue: "Port already in use"
Solution:
- Check running processes: netstat -tulpn | grep :5000
- Stop conflicting services: docker-compose down
- Change ports in docker-compose.yml if needed

Issue: "Model download fails"
Solution:
- Check internet connectivity
- Service automatically falls back to stub classifier
- Check logs: docker-compose logs inference1
- Fallback classifier provides deterministic fake predictions

Issue: "CORS errors in web UI"
Solution:
- Ensure backend services have CORS enabled (already configured)
- Check browser console for specific errors
- Verify API URLs in frontend/static_version/app.js

Issue: "Container health checks failing"
Solution:
- Wait longer for model loading (up to 2 minutes)
- Check container logs: docker logs ai-inference-1
- Verify Python dependencies installed correctly

Issue: "Web UI shows 'Service Offline'"
Solution:
- Check if inference services are running: docker ps
- Verify ports are accessible: curl http://localhost:5001/health
- Check firewall/network settings

Issue: "No server statistics displayed"
Solution:
- Verify load balancer is running: docker ps | grep load-balancer
- Check load balancer logs: docker logs ai-load-balancer
- Ensure backend servers are properly configured

Issue: "Mininet 'command not found'"
Solution:
- Install Mininet: sudo apt install mininet
- Or use Docker-only setup (skip Mininet sections)
- Run with sudo: sudo python mininet/topology.py

Issue: "Out of memory during model loading"
Solution:
- Increase Docker memory limit (Docker Desktop settings)
- Use smaller batch sizes
- Enable swap if needed

Issue: "Grafana dashboards empty"
Solution:
- Wait 2-3 minutes for metrics collection
- Check Prometheus targets: http://localhost:9090/targets
- Verify time range in Grafana (last 1 hour)

📞 Debug Commands
=================

Check Docker Status:
docker ps
docker-compose ps

Test Individual Services:
curl http://localhost:5001/health
curl http://localhost:8080/api/server-stats
curl http://localhost:9090/api/v1/targets

Check Container Resources:
docker stats

Network Connectivity:
docker network ls
docker network inspect cloud_ai-network

File an Issue:
If problems persist, collect:
1. docker-compose logs output
2. System specifications
3. Docker version info
4. Error messages from browser console

═══════════════════════════════════════════════════════════════════════════════

10. TALKING POINTS FOR FACULTY/INTERVIEWS
==========================================

🎯 Technical Architecture Discussion
=====================================

1. SDN vs Traditional Load Balancing:
"This system demonstrates Software-Defined Networking advantages over traditional load balancers. Instead of static configuration, the Ryu controller dynamically programs flow rules based on real-time server metrics, enabling intelligent traffic steering at the network layer."

2. Microservices Design Patterns:
"The architecture follows cloud-native principles with containerized services, health checks, graceful degradation, and observability-first design. Each inference service is stateless and horizontally scalable."

3. AI/ML Production Considerations:
"The system handles real-world ML deployment challenges: model loading failures (fallback classifier), GPU unavailability (CPU fallback), canary deployments (model v2), and performance monitoring."

4. Observability Strategy:
"Comprehensive monitoring with Prometheus metrics, Grafana visualization, structured logging, and real-time health checks provides full system visibility for operations and debugging."

5. Load Balancing Algorithms:
"Multiple algorithms demonstrate different optimization strategies:
- Round Robin: Simple fairness
- Least Connections: Connection-aware
- Latency Weighted: Performance-optimized
- Weighted Round Robin: Resource-aware"

🔧 Implementation Highlights
============================

1. Self-Healing Architecture:
"The system automatically handles failures - stub classifier fallback, health check recovery, traffic redistribution, and graceful error handling."

2. Frontend Technology Choices:
"Static HTML/JS frontend ensures maximum compatibility and reduces deployment complexity while providing full functionality. Progressive enhancement approach."

3. Container Orchestration:
"Docker Compose for development, with Kubernetes manifests for production scaling. Health checks, rolling updates, and service discovery."

4. Testing Strategy:
"Multi-layer testing: unit tests for algorithms, API smoke tests for functionality, load testing for performance validation."

📈 Performance Engineering
===========================

1. Bottleneck Analysis:
"Model inference is the primary bottleneck. Load balancing distributes this computational load across multiple instances."

2. Scaling Strategies:
"Horizontal scaling of inference services, vertical scaling for model loading, and caching layers for repeated requests."

3. Latency Optimization:
"Model optimization (MobileNetV2 for speed), connection pooling, and intelligent routing based on server performance."

🔮 Future Enhancements Discussion
=================================

1. Machine Learning Improvements:
"Reinforcement learning for adaptive load balancing, online learning for model updates, and A/B testing infrastructure."

2. Edge Computing:
"Distribution to edge nodes, federated learning, and mobile inference optimization."

3. Advanced SDN:
"P4 programmable switches, intent-based networking, and network function virtualization."

4. Production Readiness:
"Multi-region deployment, disaster recovery, security hardening, and compliance frameworks."

═══════════════════════════════════════════════════════════════════════════════

11. FUTURE ENHANCEMENTS
=======================

🚀 Roadmap Items
================

Phase 1: Enhanced Intelligence
- Reinforcement learning for load balancing optimization
- Drift detection for model performance monitoring
- Adaptive scaling based on request patterns

Phase 2: Edge Computing
- Mobile app with on-device inference
- Edge node deployment for reduced latency
- Federated learning across edge locations

Phase 3: Advanced Networking
- P4 programmable switch integration
- Intent-based networking policies
- Network function virtualization

Phase 4: Production Features
- Multi-tenant architecture
- Advanced security (mTLS, JWT, RBAC)
- Compliance logging and audit trails
- Disaster recovery automation

🔬 Research Opportunities
=========================

1. Adaptive Load Balancing:
"Implement Q-learning or policy gradient methods to learn optimal server selection policies based on historical performance data."

2. Predictive Scaling:
"Use time series forecasting to anticipate load spikes and preemptively scale inference capacity."

3. Model Serving Optimization:
"Investigate model quantization, distillation, and edge-specific optimizations for different deployment targets."

4. Network-Aware ML:
"Research network-conscious model partitioning and distributed inference across multiple nodes."

═══════════════════════════════════════════════════════════════════════════════

🎉 CONGRATULATIONS!
===================

You now have a fully functional AI Image Classification Service with SDN-based Load Balancing!

Key Achievements:
✅ Production-quality AI service with fallback mechanisms
✅ Intelligent SDN load balancing with multiple algorithms
✅ User-friendly web interface with real-time statistics
✅ Comprehensive observability and monitoring
✅ Automated testing and performance validation
✅ Complete documentation and troubleshooting guide

This system demonstrates enterprise-level cloud architecture, machine learning operations, and software-defined networking in a single cohesive platform.

For questions, issues, or contributions, please refer to the project documentation or open an issue in the repository.

Happy classifying! 🤖📸🚀